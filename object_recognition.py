# -*- coding: utf-8 -*-
"""Object Recognition.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1W4HDMdgRnSljxcuTbCZJ4Dtf_dzdSBn4
"""

!pip install kaggle

#configuration the path of kaggle.json file
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

#dataset api
!kaggle competitions download -c cifar-10

!ls

#extractiong the compessed dataset

from zipfile import ZipFile
dataset = '/content/cifar-10.zip'

with ZipFile(dataset, 'r') as zip:
  zip.extractall()
  print('The dataset is extracted')

!ls

!pip install py7zr

from ast import mod
import py7zr

archive = py7zr.SevenZipFile('/content/train.7z', mode='r')
archive.extractall()
archive.close()

!ls

"""**Importing the dependenceis**"""

import os
import numpy as np
import pandas as pd
from PIL import Image
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
from sklearn.model_selection import train_test_split

filenames = os.listdir('/content/train')

type(filenames)

len(filenames)

#print fast and last 5 data
print(filenames[0:5])
print(filenames[-5:])

"""**Labels Processing**"""

#read csv labels into pandas data structure
labels_df = pd.read_csv('/content/trainLabels.csv')

labels_df.shape

labels_df.head()

labels_df[labels_df['id'] == 34319]

labels_df.head(10)

labels_df.tail(10)

#unique vales in label

labels_df['label'].unique()

#values counts

labels_df['label'].value_counts()

#convert catagorical 'labels' to neumerical type

labels_dictonary = {'frog':0, 'truck':1, 'deer':2, 'automobile':3, 'bird':4, 'horse':5, 'ship':6, 'cat':7, 'dog':8, 'airplane':9}

labels = [labels_dictonary[i] for i in labels_df['label']]

print(labels[0:5])
print(labels[-5:])

#displaing a sample image
import cv2
from google.colab.patches import cv2_imshow

img = cv2.imread('/content/train/34319.png')
cv2_imshow(img)

labels_df[labels_df['id'] == 34319]

labels_df.head()

id_list = list(labels_df['id'])

print(id_list[0:5])
print(id_list[-5:])

"""**Image Processing**"""

#convert image to numpy array

train_data_folder = '/content/train'

data = []

for id in id_list:
  image = Image.open(train_data_folder + '/' + str(id) + '.png')
  image = np.array(image)
  data.append(image)

type(data)

len(data)

type(data[0])

data[0].shape

data[0]

#convert image list and label list to numpy arrray

x = np.array(data)
y = np.array(labels)

type(x)

print(x.shape)
print(y.shape)

"""**Train Test Split**"""

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=5)

print (x.shape, x_train.shape, x_test.shape)

#scaling data 0 to 1 range. so devide by 255

x_train_scaled = x_train/255
x_test_scaled = x_test/255

x_train_scaled

"""**Building Neural Network**"""

import tensorflow as tf
from tensorflow import keras

num_of_classes = 10

#setting up the layers of neural Network

model = keras.Sequential([
    keras.layers.Flatten(input_shape=(32,32,3)),
    keras.layers.Dense(128, activation='relu'),  #hiddel layer 1
    keras.layers.Dense(64, activation='relu'),    #hiddel layer 2
    keras.layers.Dense(num_of_classes, activation='softmax')    #input layer
])

#compile the neural network
model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

#training the neural network

model.fit(x_train_scaled, y_train, validation_split=0.1, epochs=10)

"""**Here used just a normal Neural Network.**

**so accuracy comes very low. only 0.497**

**Now Use ResNet50**
"""

from tensorflow.keras import Sequential, models, layers
from tensorflow.keras.layers import Dense, Dropout, Flatten
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.models import load_model
from tensorflow.keras.models import Model
from tensorflow.keras.applications.resnet50 import ResNet50
from tensorflow.keras import optimizers

convolutional_base = ResNet50(weights='imagenet', include_top=False, input_shape=(256,256,3))
#in dataset input img size is (32,32,3)
#but resnet50 worked only (256,256,3)
#so we need reshape


convolutional_base.summary()

#setting up ResNet50

number_of_classes = 10

model = models.Sequential()
#convert(32,32,3) to (256,256,3)
model.add(layers.UpSampling2D((2,2)))    #its convert (64,64,3)
model.add(layers.UpSampling2D((2,2)))    #its convert (128,128,3)
model.add(layers.UpSampling2D((2,2)))    #its convert (256,256,3)
model.add(convolutional_base)
model.add(layers.Flatten())
#hiddel layer 1
model.add(layers.BatchNormalization())
model.add(layers.Dense(128, activation='relu'))
model.add(layers.Dropout(0.5))
#hidden layer 2
model.add(layers.BatchNormalization())
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dropout(0.5))
#input layer
model.add(layers.BatchNormalization())
model.add(layers.Dense(number_of_classes, activation='softmax'))

model.compile(optimizer=optimizers.RMSprop(learning_rate=2e-5), loss='sparse_categorical_crossentropy', metrics=['acc'])

history = model.fit(x_train_scaled, y_train, validation_split=0.1, epochs=10)

#test accuracy
loss, accuracy = model.evaluate(x_test_scaled, y_test)
print('Test Accuracy = ', accuracy)

h = history

#plot the loss value
plt.plot(h.history['loss'], label='train loss')
plt.plot(['val_loss'], label='validation loss')
plt.legend()
plt.show()


#plot the accuracy value
plt.plot(h.history['acc'], label='train accuracy')
plt.plot(['val_acc'], label='validation accuracy')
plt.legend()
plt.show()

"""**Save as train Pickel File**"""

import pickle

# ... (your existing code) ...

# After training the model (after model.fit)
with open('Object Recognition.pkl', 'wb') as file:
  pickle.dump(model, file)

print("Model saved as Object Recognition_model.pkl")